# -*- coding: utf-8 -*-
"""Taller_PLN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wlNU8nfTBtDS-P2P1FGgQeuXPR_qC6CA

# Taller: Aplicaci√≥n de Machine Learning en PLN
## Integrantes:
- Nicolas Santiago Acosta Parra
- Anderson Giovany Carre√±o Rincon
- Johan Sebasti√°n Gil Salmanca
- Edison Ferney Gutierrez Buitrago

## Cargar librer√≠as necesarias
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve
from imblearn.over_sampling import RandomOverSampler
import re

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

print('Librer√≠as cargadas correctamente')

"""## 1) Cargar dataset"""

import pandas as pd
from sklearn.datasets import fetch_20newsgroups

categories_sports = ['rec.sport.baseball', 'rec.sport.hockey']
categories_non_sports = ['sci.med', 'comp.graphics']

newsgroups_train = fetch_20newsgroups(subset='train',
                                      categories=categories_sports + categories_non_sports,
                                      shuffle=True,
                                      random_state=42)

newsgroups_test = fetch_20newsgroups(subset='test',
                                     categories=categories_sports + categories_non_sports,
                                     shuffle=True,
                                     random_state=42)

df_train = pd.DataFrame({
    'text': newsgroups_train.data,
    'category': [newsgroups_train.target_names[i] for i in newsgroups_train.target],
    'is_sports': [1 if 'sport' in newsgroups_train.target_names[i] else 0
                  for i in newsgroups_train.target]
})

df_test = pd.DataFrame({
    'text': newsgroups_test.data,
    'category': [newsgroups_test.target_names[i] for i in newsgroups_test.target],
    'is_sports': [1 if 'sport' in newsgroups_test.target_names[i] else 0
                  for i in newsgroups_test.target]
})


df = pd.concat([df_train, df_test], ignore_index=True)
df.head()

print(f"\nINFORMACI√ìN DEL DATASET")
print(f"Total de documentos: {len(df)}")
print(f"Documentos de entrenamiento: {len(df_train)}")
print(f"Documentos de prueba: {len(df_test)}")
print(f"\nDistribuci√≥n por categor√≠a:")
print(df['category'].value_counts())
print(f"\nDistribuci√≥n Sports vs No-Sports:")
print(df['is_sports'].value_counts())

print("\nEJEMPLO DE TEXTO DEPORTIVO:")
print(df[df['is_sports']==1]['text'].iloc[0][:500])

"""## 2) Preprocesamiento

En este apartado se realiza la limpieza de los textos: min√∫sculas, sin s√≠mbolos, sin stopwords, tokenizamos y lematizamos.
"""

stop_words = set(stopwords.words('english'))
stemmer = SnowballStemmer('english')

def clean_text(text):
    """
    Limpia el texto eliminando elementos no deseados
    """
    text = text.lower()
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def tokenize_text(text):
    """
    Tokeniza el texto en palabras individuales
    """
    tokens = word_tokenize(text)
    return tokens

def remove_stopwords(tokens):
    """
    Elimina palabras comunes sin significado relevante
    """
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]
    return filtered_tokens

def lemmatize_tokens(tokens):
    """
    Reduce palabras a su forma base (lemma)
    """
    lemmatizer = WordNetLemmatizer()
    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]
    return lemmatized

def stem_tokens(tokens):
    """
    Reduce palabras a su ra√≠z (stem)
    """
    stemmer = PorterStemmer()
    stemmed = [stemmer.stem(token) for token in tokens]
    return stemmed

def preprocess_pipeline(text, use_lemmatization=True):
    """
    Pipeline completo de preprocesamiento
    """
    text = clean_text(text)
    tokens = tokenize_text(text)
    tokens = remove_stopwords(tokens)
    if use_lemmatization:
        tokens = lemmatize_tokens(tokens)
    else:
        tokens = stem_tokens(tokens)
    processed_text = ' '.join(tokens)

    return processed_text

df['texto_limpio'] = df['text'].apply(preprocess_pipeline)
df.head()

"""## 3) Representaci√≥n del texto

Se usa Bag of Words (CountVectorizer) y TF-IDF (TfidfVectorizer) para convertir texto a vectores.
"""

# Vectorizadores
X = df['texto_limpio']
y = df['is_sports']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

bow = CountVectorizer()
tfidf = TfidfVectorizer(max_features=500)


X_train_bow = bow.fit_transform(X_train)
X_test_bow = bow.transform(X_test)


X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

print('Tama√±o X_train_bow:', X_train_bow.shape)
print('Tama√±o X_test_bow:', X_test_bow.shape)
print('Tama√±o X_train_tfidf:', X_train_tfidf.shape)
print('Tama√±o X_test_tfidf:', X_test_tfidf.shape)

"""## Optimizaci√≥n de par√°metros"""

##OPTIMIZACI√ìN DE PAR√ÅMETROS
#
#from sklearn.model_selection import GridSearchCV
#from sklearn.naive_bayes import MultinomialNB
#from sklearn.linear_model import LogisticRegression
#from sklearn.ensemble import RandomForestClassifier
#from sklearn.metrics import classification_report
#
## =====================================================
## GRID SEARCH - NAIVE BAYES
## =====================================================
#param_nb = {
#    'alpha': [0.01, 0.1, 0.5, 1.0],
#    'fit_prior': [True, False]
#}
#
#grid_nb = GridSearchCV(
#    MultinomialNB(),
#    param_grid=param_nb,
#    cv=5,
#    scoring='f1',
#    n_jobs=-1
#)
#grid_nb.fit(X_train_tfidf, y_train)
#
#print("\nMejor configuraci√≥n Naive Bayes:", grid_nb.best_params_)
#
#
## =====================================================
## GRID SEARCH - REGRESI√ìN LOG√çSTICA
## =====================================================
#param_lr = {
#    'C': [0.01, 0.1, 1, 10],
#    'penalty': ['l2'],
#    'solver': ['liblinear', 'saga'],
#    'class_weight': [None, 'balanced']
#}
#
#grid_lr = GridSearchCV(
#    LogisticRegression(max_iter=2000, random_state=42),
#    param_grid=param_lr,
#    cv=5,
#    scoring='f1',
#    n_jobs=-1
#)
#grid_lr.fit(X_train_tfidf, y_train)
#
#print("\nMejor configuraci√≥n Regresi√≥n Log√≠stica:", grid_lr.best_params_)
#
#
## =====================================================
## GRID SEARCH - RANDOM FOREST
## =====================================================
#param_rf = {
#    'n_estimators': [100, 200, 300],
#    'max_depth': [None, 20, 50],
#    'min_samples_split': [2, 5, 10],
#    'min_samples_leaf': [1, 2, 4],
#    'max_features': ['sqrt', 'log2'],
#    'class_weight': [None, 'balanced']
#}
#
#grid_rf = GridSearchCV(
#    RandomForestClassifier(random_state=42),
#    param_grid=param_rf,
#    cv=5,
#    scoring='f1',
#    n_jobs=-1
#)
#grid_rf.fit(X_train_tfidf, y_train)
#
#print("\nMejor configuraci√≥n Random Forest:", grid_rf.best_params_)
#from sklearn.model_selection import cross_val_score
#import numpy as np

"""## 4) Divisi√≥n de datos y entrenamiento de modelos

Se divide los datos en entrenamiento y prueba para posteriormente probar los distintos modelos.

Modelos: Naive Bayes, Regresi√≥n Log√≠stica y Random Forest.

Se usa TF-IDF por defecto para entrenar.
"""

models = {
    'Naive Bayes': MultinomialNB(alpha=0.01, fit_prior=True),

    'Regresi√≥n Log√≠stica': LogisticRegression(
        C=10,
        class_weight=None,
        penalty='l2',
        solver='liblinear',
        max_iter=1000,
        random_state=42
    ),

    'Random Forest': RandomForestClassifier(class_weight=None, n_estimators=100, max_depth=50, max_features='log2', min_samples_leaf=1, min_samples_split=2, random_state=42, n_jobs=-1
    )
}
resultados = {}

for nombre, modelo in models.items():
    modelo.fit(X_train_tfidf, y_train)
    y_pred = modelo.predict(X_test_tfidf)
    resultados[nombre] = (modelo, y_pred)
    print(f"\nResultado: {nombre}")
    print(classification_report(y_test, y_pred))

"""## 5) Matrices de confusi√≥n

Se muestra una matriz por cada modelo.
"""

import matplotlib.pyplot as plt
import seaborn as sns

for nombre, (modelo, y_pred) in resultados.items():
    cm = confusion_matrix(y_test, y_pred, labels=y.unique())
    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=y.unique(), yticklabels=y.unique(), cmap='Blues')
    plt.title(f'Matriz de Confusi√≥n - {nombre}')
    plt.xlabel('Predicci√≥n')
    plt.ylabel('Real')
    plt.show()

"""## 6) WordClouds por categor√≠a

Visualizar t√©rminos frecuentes por categor√≠a.
"""

from wordcloud import WordCloud

for cat in df['category'].unique():
    texto_cat = ' '.join(df[df['category']==cat]['texto_limpio'])
    if texto_cat.strip() == '':
        continue
    wc = WordCloud(width=800, height=400).generate(texto_cat)
    plt.figure(figsize=(10,4))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'WordCloud - {cat}')
    plt.show()

"""## 7) Curva ROC / AUC

Para problemas multicategor√≠a calculamos AUC por clase usando decision_function o predict_proba cuando est√© disponible.
"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

for nombre, modelo in models.items():
    y_pred_proba = modelo.predict_proba(X_test_tfidf)[:, 1]

    # Calcular puntos de la curva ROC
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
    roc_auc = auc(fpr, tpr)

    # Graficar
    plt.figure(figsize=(6, 6))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Tasa de Falsos Positivos (1 - Especificidad)')
    plt.ylabel('Tasa de Verdaderos Positivos (Sensibilidad)')
    plt.title(f'Curva ROC - {nombre}')
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

"""## 8) Pruebas"""

class PredictText:

    def predecir_texto(self, texto):
        texto_limpio = preprocess_pipeline(texto)
        vector = tfidf.transform([texto_limpio])
        print()
        print("*"*30)
        print(f"TEXTO: '{texto}'")
        self.final_message = ""
        for nombre, modelo in models.items():
            pred = modelo.predict(vector)[0]
            answer = "Es sobre deportes" if pred == 1 else "No es sobre deportes"
            self.final_message += f"\n- Predicci√≥n de {nombre}: {answer}"
            print(f"- Predicci√≥n de {nombre}: {answer}")
        message = self.final_message
        self.final_message = ""
        return message

predict = PredictText()
print("-"*35)
print("Textos relacionados con deporte")
print("-"*35)
predict.predecir_texto("Cristiano Ronaldo scored a stunning goal in the final minutes of the game.")
predict.predecir_texto("The Lakers secured another victory with LeBron James leading the scoreboard.")
predict.predecir_texto("Serena Williams announced her retirement after an incredible career in tennis.")
predict.predecir_texto("The World Cup final attracted millions of viewers around the globe.")
predict.predecir_texto("Liverpool‚Äôs defense was solid throughout the entire match against Chelsea.")
predict.predecir_texto("The Olympic Games will feature new sports such as skateboarding and surfing.")
predict.predecir_texto("Manchester City lifted the Premier League trophy after a 3-0 win.")
predict.predecir_texto("The coach praised his team‚Äôs discipline and teamwork after the victory.")
predict.predecir_texto("A new record was set in the 100-meter sprint by an American athlete.")
predict.predecir_texto("Fans celebrated wildly as their team qualified for the Champions League final.")

print()
print("-"*35)
print("Textos No relacionados con deporte")
print("-"*35)
predict.predecir_texto("The government announced a new plan to improve public transportation.")
predict.predecir_texto("A massive wildfire spread through the northern region causing evacuations.")
predict.predecir_texto("Apple unveiled its latest iPhone model with advanced camera technology.")
predict.predecir_texto("The stock market saw a significant drop due to global economic concerns.")
predict.predecir_texto("Scientists developed a new vaccine that shows promising results.")
predict.predecir_texto("Tourism in Europe is expected to rise during the summer season.")
predict.predecir_texto("The new education policy aims to reduce inequality in rural areas.")
predict.predecir_texto("A famous actor announced his return to Hollywood after a long break.")
predict.predecir_texto("The museum opened a new exhibition about ancient Egyptian culture.")
predict.predecir_texto("Heavy rains caused flooding in several cities across the country.")

"""## 9) Conclusiones

- El sistema desarrollado logr√≥ cumplir de manera satisfactoria con el objetivo general del taller: aplicar algoritmos de Machine Learning al Procesamiento de Lenguaje Natural (PLN) para clasificar autom√°ticamente textos en ingl√©s seg√∫n su relaci√≥n con el tema de deportes.

- Durante el desarrollo del proyecto, se implementaron correctamente todas las fases del flujo de trabajo de PLN: preprocesamiento, representaci√≥n vectorial, entrenamiento de modelos, evaluaci√≥n, visualizaci√≥n y aplicaci√≥n pr√°ctica mediante una interfaz gr√°fica.

- Las m√©tricas de precisi√≥n, recall y F1-score se ubicaron en torno al 97% en los tres modelos, lo que indica un rendimiento equilibrado y estable reflejando que los algoritmos fueron capaces de capturar los patrones ling√º√≠sticos caracter√≠sticos de los textos deportivos, tales como la presencia de t√©rminos como game, match, team, player, goal, entre otros, frente a t√©rminos no relacionados con deportes.

- En las pruebas manuales con frases nuevas no incluidas en el conjunto de entrenamiento, el sistema mostr√≥ un comportamiento coherente donde la mayor√≠a de los textos deportivos fueron correctamente identificados por los tres modelos, aunque en algunos casos, con frases m√°s ambiguas o con menor contexto (‚ÄúThe Lakers secured another victory‚Ä¶‚Äù), los modelos basados en TF-IDF mostraron ciertas limitaciones para reconocer el contexto sem√°ntico, evidenciando que el sistema funciona correctamente en general, aunque su comprensi√≥n del significado a√∫n depende de la frecuencia de palabras espec√≠ficas m√°s que del contexto completo.
- Las matrices de confusi√≥n confirmaron un n√∫mero muy reducido de errores de clasificaci√≥n, lo que respalda la consistencia del modelo.
- Las nubes de palabras (WordClouds) permitieron identificar los t√©rminos m√°s frecuentes en cada categor√≠a, validando la coherencia tem√°tica entre los grupos de textos.
- Por su parte, las curvas ROC logran reflejar una excelente capacidad discriminativa entre textos deportivos y no deportivos.
"""

import tkinter as tk
from tkinter import filedialog, messagebox, scrolledtext
from PIL import Image, ImageTk
import PyPDF2
from docx import Document
import os

class SpecialButton(tk.Button):

    def __init__(self, root, text, width, primary_color, secondary_color, command=None):

        self.base_image = self.create_gradient_image(width, 36, primary_color, secondary_color)
        self.base_image_hover = self.create_gradient_image(width, 36, "#747474", "#292929")

        self.gradient_photo = ImageTk.PhotoImage(self.base_image, master=root)
        self.gradient_photo_hover = ImageTk.PhotoImage(self.base_image_hover, master=root)

        super().__init__(
            root,
            text=text,
            relief=tk.FLAT,
            bg="#FFFFFF",
            fg="white",
            activebackground=secondary_color,
            activeforeground=secondary_color,
            font=("Segoe UI", 11, "bold"),
            bd=0,
            cursor="hand2",
            image=self.gradient_photo,
            compound="center",
            padx=2,
            pady=2,
            command=command
        )

        self.secondary_color = secondary_color

        self.set_events()


    def set_events(self):
        self.bind("<Enter>", self.on_enter)
        self.bind("<Leave>", self.on_leave)

    def on_enter(self, event):
        self.config(image=self.gradient_photo_hover)

    def on_leave(self, event):
        self.config(image=self.gradient_photo)

    def create_gradient_image(self, width, height, color1, color2):
        base = Image.new('RGB', (width, height), color1)
        top = Image.new('RGB', (width, height), color2)
        mask = Image.new('L', (width, height))
        for x in range(width):
            for y in range(height):
                mask.putpixel((x, y), int(255 * (x / width)))
        base.paste(top, (0, 0), mask)
        return base

def analizar_texto():
    texto = text_box.get("1.0", tk.END).strip()
    if not texto or texto == placeholder_text:
        messagebox.showwarning("Aviso", "Por favor, ingresa un texto o carga un archivo PDF, DOCX o TXT.")
        return

    pred = predict.predecir_texto(texto)
    messagebox.showinfo("Resultado del an√°lisis", f"El texto se clasifica como: {pred}")


def cargar_archivo():
    archivo = filedialog.askopenfilename(
        filetypes=[
            ("Archivos de texto", "*.pdf *.txt *.docx"),
            ("Todos los archivos", "*.*")
        ]
    )
    if not archivo:
        return

    try:
        extension = os.path.splitext(archivo)[1].lower()
        texto = ""

        if extension == ".pdf":
            with open(archivo, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    texto += page.extract_text() or ""

        elif extension == ".txt":
            with open(archivo, "r", encoding="utf-8", errors="ignore") as f:
                texto = f.read()

        elif extension == ".docx":
            doc = Document(archivo)
            texto = "\n".join([p.text for p in doc.paragraphs])

        else:
            messagebox.showwarning("Formato no soportado", "Solo se admiten archivos PDF, TXT o DOCX.")
            return

        text_box.delete("1.0", tk.END)
        text_box.insert(tk.END, texto)
        text_box.config(fg=default_fg_color)

        if not texto:
            on_focus_out(None)
        messagebox.showinfo("Archivo cargado", f"Se ha cargado correctamente el archivo {os.path.basename(archivo)}")

    except Exception as e:
        messagebox.showerror("Error", f"No se pudo leer el archivo.\n\n{e}")

placeholder_text = "Escribe tu texto aqu√≠ o carga un archivo PDF, DOCX o TXT..."
placeholder_color = "#202020"
default_fg_color = "black"

def on_focus_in(event):
    if text_box.get("1.0", "end-1c").strip() == placeholder_text:
        text_box.delete("1.0", tk.END)
        text_box.config(fg=default_fg_color)

def on_focus_out(event):
    if not text_box.get("1.0", "end-1c").strip():
        text_box.insert("1.0", placeholder_text)
        text_box.config(fg=placeholder_color)

ventana = tk.Tk()
ventana.title("Analizador de Textos o Archivos")
ventana.configure(bg="#292929")
ventana.geometry("600x400")

tk.Label(ventana, text="Analizador de textos o archivos", font=("Arial", 14, "bold"), foreground="white", background="#292929").pack(pady=10)
tk.Label(ventana, text="El analizador determina si el texto ingresado se encuentra relacionado con los deportes.", font=("Arial", 10), foreground="#CCCCCC", background="#292929").pack(pady=(0, 5))


text_box = scrolledtext.ScrolledText(ventana, wrap=tk.WORD, width=70, height=15)
text_box.pack(padx=10, pady=10)

text_box.insert("1.0", placeholder_text)
text_box.config(fg=placeholder_color)

text_box.bind("<FocusIn>", on_focus_in)
text_box.bind("<FocusOut>", on_focus_out)

frame_botones = tk.Frame(ventana)
frame_botones.configure(bg="#292929")
frame_botones.pack(pady=10)

btn_cargar = SpecialButton(
    frame_botones,
    text="üì§ Cargar archivo",
    width=150,
    primary_color="#388E3C",
    secondary_color="#00E60B",
    command=cargar_archivo
)
btn_cargar.grid(row=0, column=0, padx=10)

btn_analizar = SpecialButton(
    frame_botones,
    text="üîé Analizar Texto",
    width=150,
    primary_color="#4A90E2",
    secondary_color="#00CFEB",
    command=analizar_texto
)
btn_analizar.grid(row=0, column=1, padx=10)

ventana.mainloop()